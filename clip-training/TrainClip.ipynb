{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91b88a9c-9559-4ad6-8a44-6aa466c33c10",
   "metadata": {},
   "source": [
    "# Training Clip\n",
    "\n",
    "- Test Clip using a test, eval dataset\n",
    "    - Will use 3000 tweets( 2500 train, 500 eval); then will try to cluster the rest of the tweets and check the results\n",
    "    - Split dataset and save it into disk\n",
    "    - Create a dataloader\n",
    "    - Training Clip using CrossEntropy Loss in the top of img and txt embeddings\n",
    "    - Check results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c30f35e-a98b-462b-b4b2-cfc7af93b5d2",
   "metadata": {},
   "source": [
    "## Split dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "774e959f-061b-4799-80af-cc246e2a2cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf2dfe74-a68d-4104-9fd2-41d076d5e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read annotation\n",
    "full_data = pd.read_pickle('../annotations/binary_annotation.pkl')\n",
    "\n",
    "\n",
    "# Select balanced train data \n",
    "# here we are considering that if any modality is relevant than the tweet itself is relevant\n",
    "positive_samples = full_data[(full_data['img'] == True) | (full_data['txt'] == True)].sample(1200, random_state=0)\n",
    "negative_samples = full_data[(full_data['img'] == False) & (full_data['txt'] == False)].sample(1800, random_state=0)\n",
    "\n",
    "use_data = pd.concat([positive_samples,negative_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e0e58ba-17a5-45e2-afe7-5161a00a0a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all data and paste it into local dir\n",
    "annotated_data = pd.read_pickle('../annotations/5k-data.pkl')\n",
    "\n",
    "# Get the rest that will to test the clusters of the trained model\n",
    "not_use_data =  pd.concat([full_data, use_data]).drop_duplicates(keep=False)\n",
    "not_use_data = annotated_data.merge(not_use_data, how='inner', on='tweet_id', suffixes=('_path', '_label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fb8b880-d9fd-4345-a5d5-0ea7c607252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide use_data into trainig and eval\n",
    "train = use_data.sample(2500, random_state=0)\n",
    "test = pd.concat([train, use_data]).drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e7fc869-de95-4cc4-b82f-a7af9352cf4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='img,txt'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[['img', 'txt']].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6761eaa2-c815-402b-876f-853663c087c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data with annotations\n",
    "train = annotated_data.merge(train, how='inner', on='tweet_id', suffixes=('_path', '_label'))\n",
    "test = annotated_data.merge(test, how='inner', on='tweet_id', suffixes=('_path', '_label'))\n",
    "\n",
    "# Save each type of data\n",
    "train.to_pickle(\"train.pkl\")\n",
    "test.to_pickle(\"test.pkl\")\n",
    "not_use_data.to_pickle(\"not_use_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2550eb33-2271-4ff4-a8c3-69d72f62eee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcadb4c6e82d4032b0a723dde770bfaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Copy Train:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75de0385455349f98dbf3a3c374f0118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Copy test:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473f3f9cd8e14fb48dd3b1c48e0bea07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Copy not_use_data:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save images of each type of data\n",
    "# train\n",
    "for _, row in tqdm(train.iterrows(), total=len(train),desc='Copy Train'):\n",
    "    src = '../annotations/'+ row['img_path']\n",
    "    dest = 'train'\n",
    "    os.makedirs(dest,exist_ok=True)\n",
    "    dest = dest +'/'+ str(Path(row['img_path']).name) \n",
    "    shutil.copy(src,dest)\n",
    "\n",
    "for _, row in tqdm(test.iterrows(), total=len(test),desc='Copy test'):\n",
    "    src = '../annotations/'+ row['img_path']\n",
    "    dest = 'test'\n",
    "    os.makedirs(dest,exist_ok=True)\n",
    "    dest = dest +'/'+ str(Path(row['img_path']).name) \n",
    "    shutil.copy(src,dest)\n",
    "    \n",
    "    \n",
    "for _, row in tqdm(not_use_data.iterrows(), total=len(not_use_data),desc='Copy not_use_data'):\n",
    "    src = '../annotations/'+ row['img_path']\n",
    "    dest = 'not_use_data'\n",
    "    os.makedirs(dest,exist_ok=True)\n",
    "    dest = dest +'/'+ str(Path(row['img_path']).name) \n",
    "    shutil.copy(src,dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7daf7d8-311b-440c-a2d9-a233e93231a4",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76722414-fd69-495f-bc94-69f219f0d451",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02365997-7c4a-4355-af33-ba63575a2923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import preprocess as text_preprocess\n",
    "from tqdm.notebook import tqdm\n",
    "train_data = pd.read_pickle('train.pkl').rename(columns={'txt':'txt_label'})\n",
    "eval_data = train_data.sample(500, random_state=0)\n",
    "train_data = pd.concat([eval_data, train_data]).drop_duplicates(keep=False)\n",
    "\n",
    "\n",
    "test_data = pd.read_pickle('test.pkl').rename(columns={'txt':'txt_label'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3868ed97-13d3-4bb9-8b37-9eefc0c1b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = []\n",
    "train_txts = []\n",
    "train_labels = []\n",
    "for _, row in train_data.iterrows():\n",
    "    \n",
    "    train_imgs.append(row['img_path'].replace(\"annotated-data\", \"train\"))\n",
    "    # Applie Preprocess stage here\n",
    "    train_txts.append(text_preprocess.pre_process(row['text'], keep_hashtag = True, keep_special_symbols = False))\n",
    "    \n",
    "    if row['img_label'] or row['txt_label']:\n",
    "        train_labels.append(1)\n",
    "    else:\n",
    "        train_labels.append(0)\n",
    "    \n",
    "eval_imgs = []\n",
    "eval_txts = []\n",
    "eval_labels = []\n",
    "for _, row in eval_data.iterrows():\n",
    "    \n",
    "    eval_imgs.append(row['img_path'].replace(\"annotated-data\", \"train\"))\n",
    "    # Applie Preprocess stage here\n",
    "    eval_txts.append(text_preprocess.pre_process(row['text'], keep_hashtag = True, keep_special_symbols = False))\n",
    "    \n",
    "    if row['img_label'] or row['txt_label']:\n",
    "        eval_labels.append(1)\n",
    "    else:\n",
    "        eval_labels.append(0)\n",
    "       \n",
    "    \n",
    "test_imgs = []\n",
    "test_txts = []\n",
    "test_labels = []\n",
    "for _, row in test_data.iterrows():\n",
    "    \n",
    "    test_imgs.append(row['img_path'].replace(\"annotated-data\", \"test\"))\n",
    "    # Applie Preprocess stage here\n",
    "    test_txts.append(text_preprocess.pre_process(row['text'], keep_hashtag = True, keep_special_symbols = False))\n",
    "    \n",
    "    if row['img_label'] or row['txt_label']:\n",
    "        test_labels.append(1)\n",
    "    else:\n",
    "        test_labels.append(0)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c052111-59fc-453f-b014-08f9ae3b6b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "IMG_FORMATS = ['JPG','JPEG', 'PNG', 'BMP', 'MPO', 'PPM', 'TIFF', 'GIF']\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# load model and image preprocessing\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "\n",
    "class CLIPDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset of Img and Text associated with an Tweet to be processed by CLIP\n",
    "    \"\"\"\n",
    "   \n",
    "    def __init__(self,\n",
    "                 image_files,\n",
    "                 texts,\n",
    "                 tokenizer,\n",
    "                 image_preprocess,\n",
    "                 labels,\n",
    "                 device\n",
    "    ):\n",
    "        \n",
    "        # Remoe invalid image_files\n",
    "        self.image_files = image_files\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_preprocess = image_preprocess\n",
    "        self.text_preprocess = text_preprocess\n",
    "        labels = [ (1,0) if l else (0,1) for l in labels ]\n",
    "        self.labels = torch.tensor(labels).unsqueeze(dim=1).type(torch.half)\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image\n",
    "        img_file = self.image_files[idx]\n",
    "        image = self.load_image(img_file)\n",
    "        preprocessed_img = self.image_preprocess(image).to(device)\n",
    "        \n",
    "        # Get Text\n",
    "        text = self.texts[idx]\n",
    "        preprocess_text = self.text_preprocess.pre_process(text)\n",
    "        preprocess_text = self.tokenizer(preprocess_text, context_length=77, truncate=True).squeeze().to(device)\n",
    "        \n",
    "        # Get Ground-truth\n",
    "        gt =  self.labels[idx].squeeze().to(device)  # 1 relevant, 0 not relevant\n",
    "                                                 \n",
    "                                                 \n",
    "        return preprocessed_img, preprocess_text, gt\n",
    "    \n",
    "    \n",
    "    def load_image(\n",
    "        self,\n",
    "        image_file,\n",
    "        target_size= None,\n",
    "        grayscale = False,\n",
    "        img_formats = IMG_FORMATS,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Load an image given its path. Returns an array version of optionally resized and grayed image. Only allows images\n",
    "        of types described by img_formats argument.\n",
    "        Args:\n",
    "            image_file: Path to the image file.\n",
    "            target_size: Size to resize the input image to.\n",
    "            grayscale: A boolean indicating whether to grayscale the image.\n",
    "            img_formats: List of allowed image formats that can be loaded.\n",
    "\n",
    "        Original Method from https://github.com/idealo/imagededup/blob/3465540cc5c8fdf9254aff76069e28641dfc515f/imagededup/utils/image_utils.py\n",
    "        \"\"\"\n",
    "        try:\n",
    "            img = Image.open(image_file)\n",
    "\n",
    "            # validate image format\n",
    "            if img.format not in img_formats:\n",
    "                #logger.warning(f'Invalid image format {img.format}!')\n",
    "                return None\n",
    "\n",
    "            else:\n",
    "                if img.mode != 'RGB':\n",
    "                    # convert to RGBA first to avoid warning\n",
    "                    # we ignore alpha channel if available\n",
    "                    img = img.convert('RGBA').convert('RGB')\n",
    "\n",
    "                return img\n",
    "\n",
    "        except Exception as e:\n",
    "            #logger.warning(f'Invalid image file {image_file}:\\n{e}')\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a97dea2f-782f-4a9c-9c59-36366bf64f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 224, 224])\n",
      "torch.Size([10, 77])\n",
      "torch.Size([10, 2])\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# Test Dataloader #\n",
    "###################\n",
    "dataset = CLIPDataset(train_imgs, train_txts, clip.tokenize, preprocess, train_labels, device)\n",
    "train_loader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "preprocessed_img, preprocess_text, gt = next(iter(train_loader))\n",
    "print(preprocessed_img.shape)\n",
    "print(preprocess_text.shape)\n",
    "print(gt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c997fba6-1dc4-4b81-9a10-ad8684e33142",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07ab5138-5199-4bf8-af55-58cde75f3345",
   "metadata": {
    "tags": []
   },
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecf5c1df-40b3-4bc4-bb5d-ac897def75b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvFModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,model,embedd_size):\n",
    "\n",
    "        super(EvFModel, self).__init__()\n",
    "        \n",
    "        self.clip_model = model\n",
    "        self.layer1 = torch.nn.Linear(in_features=embedd_size, out_features=embedd_size).type(torch.half)\n",
    "        self.layer2 = torch.nn.Linear(in_features=embedd_size, out_features=2).type(torch.half)\n",
    "        \n",
    "        \n",
    "        # freeze entire method, but token embedding and ln_final\n",
    "        #### next(model.named_parameters())\n",
    "        #### model.state_dict().keys()\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # self.clip_model.token_embedding.weight.requires_grad = True\n",
    "        self.clip_model.ln_final.weight.requires_grad = True\n",
    "        self.clip_model.ln_final.bias.requires_grad = True\n",
    "        \n",
    "\n",
    "    def forward(self, img_x, txt_x):\n",
    "        \n",
    "        \n",
    "        # get img embeddings\n",
    "        img_features = self.clip_model.encode_image(img_x)\n",
    "        \n",
    "        # get txt embeddings\n",
    "        txt_features = self.clip_model.encode_text(txt_x)\n",
    "        \n",
    "        # make combinantion\n",
    "        # Simplest combination ever\n",
    "        x = img_features + txt_features\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        # set linear layer\n",
    "        \n",
    "        # set linear layer\n",
    "        x = self.layer1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9494a199-fba9-4cbd-b947-245937081666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "LR=0.1\n",
    "HIDDEN_UNITS = 512\n",
    "EPOCH = 20\n",
    "BATCH_SIZE= 2048\n",
    "\n",
    "# Model\n",
    "ef_model = EvFModel(model, HIDDEN_UNITS).to(device)\n",
    "optimizer  = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#  Data\n",
    "train_dataset = CLIPDataset(train_imgs, train_txts, clip.tokenize, preprocess, train_labels, device)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "# Eval\n",
    "eval_dataset = CLIPDataset(eval_imgs, eval_txts, clip.tokenize, preprocess, eval_labels, device)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Test\n",
    "test_dataset = CLIPDataset(test_imgs, test_txts, clip.tokenize, preprocess, test_labels, device)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26d32cea-e8a1-4722-9ecd-9a383cf61801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Trainable Parameters: 264706\n"
     ]
    }
   ],
   "source": [
    "# Trainable parameters\n",
    "# Ref https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/9\n",
    "num_param = sum(p.numel() for p in ef_model.parameters() if p.requires_grad)\n",
    "print(f\"Number of Trainable Parameters: {num_param}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9f07a8-faa0-40d8-a325-18444ea597d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(img_x, txt_x, y):\n",
    "\n",
    "    ef_model.train()\n",
    "    ef_model.zero_grad()\n",
    "\n",
    "    # Forward\n",
    "    y_logits = ef_model(img_x.to(device), txt_x.to(device))\n",
    "    # Calcula loss\n",
    "    loss = loss_func(y_logits, y.to(device))\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Calcula Acurácia\n",
    "    y_hat = torch.argmax(y_logits, dim=1)\n",
    "    y_ans =  torch.argmax(y, dim=1)\n",
    "    accuracy = (y_hat == y_ans).type(torch.float).mean()\n",
    "    \n",
    "    return loss.item(), accuracy.item()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a100b89-3f5f-4faf-85b5-3fe34097e98b",
   "metadata": {},
   "source": [
    "IMG_X ,TXT_X ,Y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c1f8029-6775-4ef2-ae21-3b84c49ce639",
   "metadata": {},
   "source": [
    "overfit_loss = []\n",
    "for epoch in tqdm(range(1000),leave=True):\n",
    "    train_loss, _ = train_step(IMG_X.to(device), TXT_X.to(device), Y.to(device))\n",
    "    # Save loss\n",
    "    overfit_loss.append(train_loss)\n",
    "\n",
    "plt.plot(overfit_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fb42940-aaa2-44f7-9fc0-81bf403e2b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db5323c-09cd-4dba-87ce-9d9f92b79ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d59a24e03124f689482d2e3ab5b7aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train_step(img_x, txt_x, y):\n",
    "    # Tranning Function\n",
    "\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Forward\n",
    "    y_logits = ef_model(img_x.to(device), txt_x.to(device))\n",
    "    # loss\n",
    "    loss = loss_func(y_logits, y.to(device))\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Accuracy\n",
    "    y_hat = torch.argmax(y_logits, dim=1)\n",
    "    y_ans =  torch.argmax(y, dim=1)\n",
    "    accuracy = (y_hat == y_ans).type(torch.float).mean()\n",
    "    \n",
    "    return loss.item(), accuracy.item()\n",
    "\n",
    "def validation_step(img_x, txt_x, y):\n",
    "    # Validation Function\n",
    "    y_logits = ef_model(img_x.to(device), txt_x.to(device))\n",
    "    loss = loss_func(y_logits, y)\n",
    "    \n",
    "    # Accuracy\n",
    "    y_hat = torch.argmax(y_logits, dim=1)\n",
    "    y_ans =  torch.argmax(y, dim=1)\n",
    "    accuracy = (y_hat == y_ans).type(torch.float).mean()\n",
    "    \n",
    "    return loss.item(), accuracy.item()\n",
    "\n",
    "# Start SummuaryWriter Tensorboard\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(EPOCH),leave=True):\n",
    "        \n",
    "    train_loss, train_acc = np.average([\n",
    "        train_step(x.to(device),mask.to(device), y.to(device)) \n",
    "        for x,mask, y in train_loader\n",
    "    ],axis=0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        valid_loss, valid_acc = np.average([\n",
    "            validation_step(x.to(device), mask.to(device), y.to(device))\n",
    "            for x, mask, y in eval_loader\n",
    "        ],axis=0)\n",
    "    \n",
    "    # Grava resultado para visualização no Tensorboard\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/validation', valid_loss, epoch)\n",
    "\n",
    "    writer.add_scalar('Acc/train', train_acc, epoch)\n",
    "    writer.add_scalar('Acc/validation', valid_acc, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06d1256-13f1-44c1-8dab-79fe60839387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (filter)",
   "language": "python",
   "name": "filter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
