{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91b88a9c-9559-4ad6-8a44-6aa466c33c10",
   "metadata": {},
   "source": [
    "# Training Clip\n",
    "\n",
    "- Test Clip using a test, eval dataset\n",
    "    - Will use 3100 tweets(2600 train, 500 eval); then will try to cluster the rest of the tweets and check the results\n",
    "    - Split dataset and save it into disk\n",
    "    - Create a dataloader\n",
    "    - Training Clip using CrossEntropy Loss in the top of img and txt embeddings\n",
    "    - Check results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c30f35e-a98b-462b-b4b2-cfc7af93b5d2",
   "metadata": {},
   "source": [
    "## Split dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "774e959f-061b-4799-80af-cc246e2a2cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf2dfe74-a68d-4104-9fd2-41d076d5e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read annotation\n",
    "full_data = pd.read_pickle('../annotations/binary_annotation.pkl')\n",
    "\n",
    "\n",
    "# Select balanced train data \n",
    "# here we are considering that if any modality is relevant than the tweet itself is relevant\n",
    "positive_samples = full_data[(full_data['img'] == True) | (full_data['txt'] == True)].sample(1216, random_state=0)\n",
    "negative_samples = full_data[(full_data['img'] == False) & (full_data['txt'] == False)].sample(1884, random_state=0)\n",
    "\n",
    "use_data = pd.concat([positive_samples,negative_samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e0e58ba-17a5-45e2-afe7-5161a00a0a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all data and paste it into local dir\n",
    "annotated_data = pd.read_pickle('../annotations/5k-data.pkl')\n",
    "\n",
    "# Get the rest that will to test the clusters of the trained model\n",
    "cluster_data =  pd.concat([full_data, use_data]).drop_duplicates(keep=False)\n",
    "cluster_data = annotated_data.merge(cluster_data, how='inner', on='tweet_id', suffixes=('_path', '_label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fb8b880-d9fd-4345-a5d5-0ea7c607252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide use_data into trainig and eval\n",
    "train = use_data.sample(2600, random_state=0)\n",
    "test = pd.concat([train, use_data]).drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e7fc869-de95-4cc4-b82f-a7af9352cf4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='img,txt'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_data[['img', 'txt']].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6761eaa2-c815-402b-876f-853663c087c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data with annotations\n",
    "train = annotated_data.merge(train, how='inner', on='tweet_id', suffixes=('_path', '_label'))\n",
    "test = annotated_data.merge(test, how='inner', on='tweet_id', suffixes=('_path', '_label'))\n",
    "\n",
    "# Save each type of data\n",
    "train.to_pickle(\"train.pkl\")\n",
    "test.to_pickle(\"test.pkl\")\n",
    "cluster_data.to_pickle(\"cluster_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2550eb33-2271-4ff4-a8c3-69d72f62eee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d7e80e6af3d452199635896641a216a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Copy Train:   0%|          | 0/2600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd60f8f874b047b581735fbe56f0b9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Copy test:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd1780106e94d7e89dd98d609f8d42c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Copy not_use_data:   0%|          | 0/1900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save images of each type of data\n",
    "# train\n",
    "for _, row in tqdm(train.iterrows(), total=len(train),desc='Copy Train'):\n",
    "    src = '../annotations/'+ row['img_path']\n",
    "    dest = 'train'\n",
    "    os.makedirs(dest,exist_ok=True)\n",
    "    dest = dest +'/'+ str(Path(row['img_path']).name) \n",
    "    shutil.copy(src,dest)\n",
    "\n",
    "for _, row in tqdm(test.iterrows(), total=len(test),desc='Copy test'):\n",
    "    src = '../annotations/'+ row['img_path']\n",
    "    dest = 'test'\n",
    "    os.makedirs(dest,exist_ok=True)\n",
    "    dest = dest +'/'+ str(Path(row['img_path']).name) \n",
    "    shutil.copy(src,dest)\n",
    "    \n",
    "    \n",
    "for _, row in tqdm(cluster_data.iterrows(), total=len(cluster_data),desc='Copy cluster_data'):\n",
    "    src = '../annotations/'+ row['img_path']\n",
    "    dest = 'cluster_data'\n",
    "    os.makedirs(dest,exist_ok=True)\n",
    "    dest = dest +'/'+ str(Path(row['img_path']).name) \n",
    "    shutil.copy(src,dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7daf7d8-311b-440c-a2d9-a233e93231a4",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76722414-fd69-495f-bc94-69f219f0d451",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02365997-7c4a-4355-af33-ba63575a2923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import preprocess as text_preprocess\n",
    "from tqdm.notebook import tqdm\n",
    "train_data = pd.read_pickle('train.pkl').rename(columns={'txt':'txt_label'})\n",
    "eval_data = train_data.sample(200, random_state=0)\n",
    "train_data = pd.concat([eval_data, train_data]).drop_duplicates(keep=False)\n",
    "\n",
    "\n",
    "test_data = pd.read_pickle('test.pkl').rename(columns={'txt':'txt_label'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3868ed97-13d3-4bb9-8b37-9eefc0c1b4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = []\n",
    "train_txts = []\n",
    "train_labels = []\n",
    "for _, row in train_data.iterrows():\n",
    "    \n",
    "    train_imgs.append(row['img_path'].replace(\"annotated-data\", \"train\"))\n",
    "    # Applie Preprocess stage here\n",
    "    train_txts.append(text_preprocess.pre_process(row['text'], keep_hashtag = True, keep_special_symbols = False))\n",
    "    \n",
    "    if row['img_label'] or row['txt_label']:\n",
    "        train_labels.append(1)\n",
    "    else:\n",
    "        train_labels.append(0)\n",
    "    \n",
    "eval_imgs = []\n",
    "eval_txts = []\n",
    "eval_labels = []\n",
    "for _, row in eval_data.iterrows():\n",
    "    \n",
    "    eval_imgs.append(row['img_path'].replace(\"annotated-data\", \"train\"))\n",
    "    # Applie Preprocess stage here\n",
    "    eval_txts.append(text_preprocess.pre_process(row['text'], keep_hashtag = True, keep_special_symbols = False))\n",
    "    \n",
    "    if row['img_label'] or row['txt_label']:\n",
    "        eval_labels.append(1)\n",
    "    else:\n",
    "        eval_labels.append(0)\n",
    "       \n",
    "    \n",
    "test_imgs = []\n",
    "test_txts = []\n",
    "test_labels = []\n",
    "for _, row in test_data.iterrows():\n",
    "    \n",
    "    test_imgs.append(row['img_path'].replace(\"annotated-data\", \"test\"))\n",
    "    # Applie Preprocess stage here\n",
    "    test_txts.append(text_preprocess.pre_process(row['text'], keep_hashtag = True, keep_special_symbols = False))\n",
    "    \n",
    "    if row['img_label'] or row['txt_label']:\n",
    "        test_labels.append(1)\n",
    "    else:\n",
    "        test_labels.append(0)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c052111-59fc-453f-b014-08f9ae3b6b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "IMG_FORMATS = ['JPG','JPEG', 'PNG', 'BMP', 'MPO', 'PPM', 'TIFF', 'GIF']\n",
    "\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# load model and image preprocessing\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "\n",
    "class CLIPDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset of Img and Text associated with an Tweet to be processed by CLIP\n",
    "    \"\"\"\n",
    "   \n",
    "    def __init__(self,\n",
    "                 image_files,\n",
    "                 texts,\n",
    "                 tokenizer,\n",
    "                 image_preprocess,\n",
    "                 labels,\n",
    "                 device\n",
    "    ):\n",
    "        \n",
    "        self.image_preprocess = image_preprocess\n",
    "        self.text_preprocess = text_preprocess\n",
    "        self.image_files = image_files\n",
    "        self.images = [self.image_preprocess(self.load_image(i)) for i in tqdm(self.image_files)]\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "       \n",
    "        labels = [ (1,0) if l else (0,1) for l in labels ]\n",
    "        self.labels = torch.tensor(labels).unsqueeze(dim=1).type(torch.half)\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image\n",
    "        image = self.images[idx]\n",
    "        preprocessed_img = image.to(device)\n",
    "        \n",
    "        # Get Text\n",
    "        text = self.texts[idx]\n",
    "        preprocess_text = self.text_preprocess.pre_process(text)\n",
    "        preprocess_text = self.tokenizer(preprocess_text, context_length=77, truncate=True).squeeze().to(device)\n",
    "        \n",
    "        # Get Ground-truth\n",
    "        gt =  self.labels[idx].squeeze().to(device)  # 1 relevant, 0 not relevant\n",
    "                                                 \n",
    "                                                 \n",
    "        return preprocessed_img, preprocess_text, gt\n",
    "    \n",
    "    \n",
    "    def load_image(\n",
    "        self,\n",
    "        image_file,\n",
    "        target_size= None,\n",
    "        grayscale = False,\n",
    "        img_formats = IMG_FORMATS,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Load an image given its path. Returns an array version of optionally resized and grayed image. Only allows images\n",
    "        of types described by img_formats argument.\n",
    "        Args:\n",
    "            image_file: Path to the image file.\n",
    "            target_size: Size to resize the input image to.\n",
    "            grayscale: A boolean indicating whether to grayscale the image.\n",
    "            img_formats: List of allowed image formats that can be loaded.\n",
    "\n",
    "        Original Method from https://github.com/idealo/imagededup/blob/3465540cc5c8fdf9254aff76069e28641dfc515f/imagededup/utils/image_utils.py\n",
    "        \"\"\"\n",
    "        try:\n",
    "            img = Image.open(image_file)\n",
    "\n",
    "            # validate image format\n",
    "            if img.format not in img_formats:\n",
    "                #logger.warning(f'Invalid image format {img.format}!')\n",
    "                return None\n",
    "\n",
    "            else:\n",
    "                if img.mode != 'RGB':\n",
    "                    # convert to RGBA first to avoid warning\n",
    "                    # we ignore alpha channel if available\n",
    "                    img = img.convert('RGBA').convert('RGB')\n",
    "\n",
    "                return img\n",
    "\n",
    "        except Exception as e:\n",
    "            #logger.warning(f'Invalid image file {image_file}:\\n{e}')\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a97dea2f-782f-4a9c-9c59-36366bf64f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518326c651b443559a359c0b065a2693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 224, 224])\n",
      "torch.Size([10, 77])\n",
      "torch.Size([10, 2])\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# Test Dataloader #\n",
    "###################\n",
    "dataset = CLIPDataset(train_imgs, train_txts, clip.tokenize, preprocess, train_labels, device)\n",
    "train_loader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "preprocessed_img, preprocess_text, gt = next(iter(train_loader))\n",
    "print(preprocessed_img.shape)\n",
    "print(preprocess_text.shape)\n",
    "print(gt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c997fba6-1dc4-4b81-9a10-ad8684e33142",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fb92c98-c627-4bbf-a5c2-76badc29cd19",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c31e8841-d923-43cf-877f-1ea8b8f15bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 154])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([preprocess_text, preprocess_text], dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecf5c1df-40b3-4bc4-bb5d-ac897def75b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvFModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,model,embedd_size):\n",
    "\n",
    "        super(EvFModel, self).__init__()\n",
    "        \n",
    "        self.clip_model = model\n",
    "        self.layer1 = torch.nn.Linear(in_features=embedd_size, out_features=embedd_size).type(torch.half)\n",
    "        self.layer2 = torch.nn.Linear(in_features=embedd_size, out_features=embedd_size).type(torch.half)\n",
    "        self.classification = torch.nn.Linear(in_features=embedd_size, out_features=2).type(torch.half)\n",
    "        \n",
    "        \n",
    "        # freeze entire method, but token embedding and ln_final\n",
    "        #### next(model.named_parameters())\n",
    "        #### model.state_dict().keys()\n",
    "      \n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        ## self.clip_model.token_embedding.weight.requires_grad = True\n",
    "        self.clip_model.ln_final.weight.requires_grad = True\n",
    "        self.clip_model.ln_final.bias.requires_grad = True\n",
    "\n",
    "    def forward(self, img_x, txt_x):\n",
    "        \n",
    "        \n",
    "        # get img embeddings\n",
    "        img_features = self.clip_model.encode_image(img_x)\n",
    "        \n",
    "        # get txt embeddings\n",
    "        txt_features = self.clip_model.encode_text(txt_x)\n",
    "        \n",
    "        # make combinantion\n",
    "        # Simplest combination ever\n",
    "        x = img_features + txt_features\n",
    "        # x = torch.cat([img_features, txt_features], dim=-1)\n",
    "        \n",
    "        \n",
    "        # set linear layer\n",
    "        x = self.layer1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.classification(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9494a199-fba9-4cbd-b947-245937081666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db4b7f86b6884e06ad278819afb8e9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e569e056d3ff439aa8cc6b864a1f9a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "LR=0.05\n",
    "HIDDEN_UNITS = 512\n",
    "EPOCH = 100\n",
    "BATCH_SIZE= 4096\n",
    "\n",
    "# Model\n",
    "ef_model = EvFModel(model, HIDDEN_UNITS).to(device)\n",
    "optimizer  = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#  Data\n",
    "train_dataset = CLIPDataset(train_imgs, train_txts, clip.tokenize, preprocess, train_labels, device)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "# Eval\n",
    "eval_dataset = CLIPDataset(eval_imgs, eval_txts, clip.tokenize, preprocess, eval_labels, device)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26d32cea-e8a1-4722-9ecd-9a383cf61801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Trainable Parameters: 527362\n"
     ]
    }
   ],
   "source": [
    "# Trainable parameters\n",
    "# Ref https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/9\n",
    "num_param = sum(p.numel() for p in ef_model.parameters() if p.requires_grad)\n",
    "print(f\"Number of Trainable Parameters: {num_param}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba9f07a8-faa0-40d8-a325-18444ea597d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(img_x, txt_x, y):\n",
    "\n",
    "    ef_model.train()\n",
    "    ef_model.zero_grad()\n",
    "\n",
    "    # Forward\n",
    "    y_logits = ef_model(img_x.to(device), txt_x.to(device))\n",
    "    # Calcula loss\n",
    "    loss = loss_func(y_logits, y.to(device))\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Calcula AcurÃ¡cia\n",
    "    y_hat = torch.argmax(y_logits, dim=1)\n",
    "    y_ans =  torch.argmax(y, dim=1)\n",
    "    accuracy = (y_hat == y_ans).type(torch.float).mean()\n",
    "    \n",
    "    return loss.item(), accuracy.item()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a100b89-3f5f-4faf-85b5-3fe34097e98b",
   "metadata": {},
   "source": [
    "IMG_X ,TXT_X ,Y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c1f8029-6775-4ef2-ae21-3b84c49ce639",
   "metadata": {},
   "source": [
    "overfit_loss = []\n",
    "for epoch in tqdm(range(1000),leave=True):\n",
    "    train_loss, _ = train_step(IMG_X.to(device), TXT_X.to(device), Y.to(device))\n",
    "    # Save loss\n",
    "    overfit_loss.append(train_loss)\n",
    "\n",
    "plt.plot(overfit_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fb42940-aaa2-44f7-9fc0-81bf403e2b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4db5323c-09cd-4dba-87ce-9d9f92b79ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a392b434674f83b58902c7e2ca7832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train_step(img_x, txt_x, y):\n",
    "    # Tranning Function\n",
    "\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Forward\n",
    "    y_logits = ef_model(img_x.to(device), txt_x.to(device))\n",
    "    # loss\n",
    "    loss = loss_func(y_logits, y.to(device))\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Accuracy\n",
    "    y_hat = torch.argmax(y_logits, dim=1)\n",
    "    y_ans =  torch.argmax(y, dim=1)\n",
    "    accuracy = (y_hat == y_ans).type(torch.float).mean()\n",
    "    \n",
    "    return loss.item(), accuracy.item()\n",
    "\n",
    "def validation_step(img_x, txt_x, y):\n",
    "    # Validation Function\n",
    "    y_logits = ef_model(img_x.to(device), txt_x.to(device))\n",
    "    loss = loss_func(y_logits, y)\n",
    "    \n",
    "    # Accuracy\n",
    "    y_hat = torch.argmax(y_logits, dim=1)\n",
    "    y_ans =  torch.argmax(y, dim=1)\n",
    "    accuracy = (y_hat == y_ans).type(torch.float).mean()\n",
    "    \n",
    "    return loss.item(), accuracy.item()\n",
    "\n",
    "# Start SummuaryWriter Tensorboard\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(EPOCH),leave=True):\n",
    "        \n",
    "    train_loss, train_acc = np.average([\n",
    "        train_step(x.to(device),mask.to(device), y.to(device)) \n",
    "        for x,mask, y in train_loader\n",
    "    ],axis=0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        valid_loss, valid_acc = np.average([\n",
    "            validation_step(x.to(device), mask.to(device), y.to(device))\n",
    "            for x, mask, y in eval_loader\n",
    "        ],axis=0)\n",
    "    \n",
    "    # Grava resultado para visualizaÃ§Ã£o no Tensorboard\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/validation', valid_loss, epoch)\n",
    "\n",
    "    writer.add_scalar('Acc/train', train_acc, epoch)\n",
    "    writer.add_scalar('Acc/validation', valid_acc, epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6157ed8b-786b-465f-95ee-47904d3010e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a08a9875-f35b-493d-b8c0-6fc522b419ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84b3596d642c48038131e0801ed671da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc 0.83\n",
      "Test Loss 0.43420263671875\n"
     ]
    }
   ],
   "source": [
    "# Freeze model\n",
    "ef_model.eval()\n",
    "\n",
    "# Test\n",
    "test_dataset = CLIPDataset(test_imgs, test_txts, clip.tokenize, preprocess, test_labels, device)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "def test_step(img_x, txt_x, y):\n",
    "    # FunÃ§Ã£o de Teste\n",
    "    y_logits =  ef_model(img_x.to(device), txt_x.to(device))\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_func(y_logits, y)\n",
    "    \n",
    "    # Calcula AcurÃ¡cia\n",
    "    y_hat = torch.argmax(y_logits, dim=1)\n",
    "    y_ans =  torch.argmax(y, dim=1)\n",
    "    accuracy = (y_hat == y_ans).type(torch.float).mean()\n",
    "    \n",
    "    return loss.item(), accuracy.item()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_loss, test_acc = np.average([\n",
    "        test_step(x.to(device), mask.to(device), y.to(device))\n",
    "        for x, mask, y in test_loader\n",
    "    ],axis=0)\n",
    "\n",
    "print(\"Test Acc\", test_acc)\n",
    "print(\"Test Loss\", test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "545dfd00-2bf1-48f1-a714-83f80f2300d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save Model\n",
    "torch.save(ef_model.state_dict(), f'ef_model-ACC-{test_acc}.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cad984-6120-446f-a189-0ad3c8ea1f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "\n",
    "# Hyperparameters\n",
    "LR=0.1\n",
    "HIDDEN_UNITS = 512\n",
    "EPOCH = 100\n",
    "BATCH_SIZE= 4096\n",
    "\n",
    "# Model\n",
    "ef_model = EvFModel(model, HIDDEN_UNITS).to(device)\n",
    "ef_model.load_state_dict(torch.load('ef_model-ACC-0.844.model'))\n",
    "ef_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba77149b-73eb-439d-8c46-42571c6c7d9d",
   "metadata": {},
   "source": [
    "# Next Step is extract the features from the not_used_data and visualize its cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cac43c-5722-476d-bf0f-b9779c86960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_X ,TXT_X ,Y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ce3b252-65a6-4dcb-885e-8bc1b5921c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class featX(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,model):\n",
    "\n",
    "        super(featX, self).__init__()\n",
    "        \n",
    "        feature_extractor = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "        self.clip_model = feature_extractor[0]\n",
    "        self.feats = feature_extractor[1]\n",
    "        \n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        for param in self.feats.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, img_x, txt_x):\n",
    "        \n",
    "        \n",
    "        # get img embeddings\n",
    "        img_features = self.clip_model.encode_image(img_x)\n",
    "        \n",
    "        # get txt embeddings\n",
    "        txt_features = self.clip_model.encode_text(txt_x)\n",
    "        \n",
    "        # make combinantion\n",
    "        # Simplest combination ever\n",
    "        x = img_features + txt_features\n",
    "        x = self.feats(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3cd5e59b-5034-4d00-a19b-8f042f3b3d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3149, -0.3792,  0.9390,  ...,  0.5029,  1.2715, -1.2559],\n",
       "        [-0.0446,  0.1746,  1.1045,  ...,  0.2695,  1.4258, -0.4944],\n",
       "        [ 0.1394, -0.0737,  0.9102,  ...,  0.4424,  0.8730, -0.2104],\n",
       "        ...,\n",
       "        [ 0.2629, -0.7715,  0.8525,  ...,  0.1625,  2.1699,  0.1089],\n",
       "        [ 0.1946,  0.3721,  1.2988,  ...,  0.7896,  2.3574, -0.6270],\n",
       "        [ 1.7549, -0.3376,  0.5566,  ..., -0.7622,  1.6777,  0.1235]],\n",
       "       device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_extractor = featX(ef_model).to(device)\n",
    "feature_extractor(IMG_X ,TXT_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede113ec-e8ba-4120-acd4-b30e9a64bff8",
   "metadata": {},
   "source": [
    "# Extract features from not_use_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45663ac5-ba16-49f3-a077-24b50cdcb4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_data = pd.read_pickle('cluster_data.pkl').rename(columns={'txt':'txt_label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a61a91c-f5d1-4178-a5ff-1041b3bb9050",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_imgs = []\n",
    "cluster_txts = []\n",
    "cluster_labels = []\n",
    "for _, row in cluster_data.iterrows():\n",
    "    \n",
    "    cluster_imgs.append(row['img_path'].replace(\"annotated-data\", \"cluster_data/\"))\n",
    "    # Applie Preprocess stage here\n",
    "    cluster_txts.append(text_preprocess.pre_process(row['text'], keep_hashtag = True, keep_special_symbols = False))\n",
    "    \n",
    "    if row['img_label'] or row['txt_label']:\n",
    "        cluster_labels.append(1)\n",
    "    else:\n",
    "        cluster_labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f53ccf4c-5900-40d6-8a63-b54b3470659b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bc827bfe6584346b9f94011ee596f1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster_dataset = CLIPDataset(cluster_imgs, cluster_txts, clip.tokenize, preprocess, cluster_labels, device)\n",
    "cluster_loader = DataLoader(cluster_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51005620-d8c0-45f8-b05e-415a82eea636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc2027cc97146b49090418c2b30866c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def normalize_vector(\n",
    "    vec: np.ndarray,\n",
    "    axis: int = -1,\n",
    "    order: int = 2\n",
    "):\n",
    "    \"\"\"\n",
    "    Normalize a vector.\n",
    "    If you pass a Matrix, each line of the matrix will be \n",
    "    considerated as a different vector and normalized as it.\n",
    "    \"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(vec, order, axis))\n",
    "    l2[l2==0] = 1\n",
    "    return vec / np.expand_dims(l2, axis)\n",
    "\n",
    "embedded_vectors = []\n",
    "feature_extractor = featX(ef_model).to(device)\n",
    "for IMG_X ,TXT_X ,Y in tqdm(cluster_loader, total=np.ceil(len(cluster_dataset) / BATCH_SIZE).astype(int)):\n",
    "    imgs = IMG_X.to(device)\n",
    "    txts = TXT_X.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = feature_extractor(imgs,txts)\n",
    "        output = output.squeeze().cpu().numpy()\n",
    "\n",
    "    output = normalize_vector(output)\n",
    "\n",
    "    embedded_vectors += output.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "053d9b51-24fe-4d8c-9d7f-9bb3dbd58221",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_data['clip_embeddings'] = embedded_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "521e8e6a-9b49-4b5c-9bc2-621f938644cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_data.to_pickle('trained_clip.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fde208c-e6ef-4c1b-a28b-9ccfe3fd035b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (filter)",
   "language": "python",
   "name": "filter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
